{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b58a7e",
   "metadata": {},
   "source": [
    "# Item-Item\n",
    "\n",
    "This file is an extension of the previously existing recommender system that we have replicated. In this, we will add an extra layer of recommendation through an item-item algorithm, with access to song features from the spotify 1m song dataset.\n",
    "\n",
    "While it does not contain a ton of song data, it has tag data on roughly ~50,000 popular songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd5d632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af40723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_challenge_set():\n",
    "    ret = []\n",
    "\n",
    "    f = open('challenge_set.json')\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    mpd_slice = json.loads(js)\n",
    "    playlists = mpd_slice['playlists']\n",
    "    for playlist in playlists:\n",
    "        ret.append(list(map(lambda x: x['track_uri'][14:], playlist['tracks'])))\n",
    "    return ret\n",
    "\n",
    "# index this at playlists IDs, should have 0 - 1000\n",
    "challenge_set_songs = load_challenge_set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2dd759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read song info and create a mapping between spotify id and index\n",
    "song_info = pd.read_csv('data/Music Info.csv')\n",
    "song_info_2 = pd.read_csv('data/SpotifyAudioFeaturesApril2019.csv')\n",
    "\n",
    "song_info_2 = song_info_2.rename(columns={\n",
    "    'track_id': 'spotify_id', \n",
    "    'artist_name': 'artist', \n",
    "    'track_name': 'name'\n",
    "})\n",
    "\n",
    "\n",
    "merged_df = pd.merge(song_info, song_info_2, on='spotify_id', how='outer')\n",
    "weird_cols = [\"name\", \"artist\", \"duration_ms\", \"danceability\", \"energy\",\n",
    "              \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\",\n",
    "              \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"]\n",
    "\n",
    "# get rid of x/y columns\n",
    "for col in weird_cols:\n",
    "    x_col = col + '_x'\n",
    "    y_col = col + '_y'\n",
    "\n",
    "    merged_df[col] = merged_df[x_col].combine_first(merged_df[y_col])\n",
    "    merged_df.drop(columns=[x_col, y_col], inplace=True)\n",
    "\n",
    "\n",
    "# make a mapping between spotify id and index\n",
    "uri_to_idx = {}\n",
    "for i, row in merged_df.iterrows():\n",
    "    uri_to_idx[row[\"spotify_id\"]] = i\n",
    "\n",
    "# get the song features\n",
    "metrics = ['key', 'loudness', 'mode', 'danceability', 'energy', 'speechiness', 'acousticness', \n",
    "           'instrumentalness','liveness', 'valence', 'tempo', 'time_signature']\n",
    "song_features = merged_df[metrics]\n",
    "# song_features = song_info_2[metrics]\n",
    "\n",
    "# not every feature is 0-1, use a scaler to normalize\n",
    "scaler = StandardScaler()\n",
    "song_features = pd.DataFrame(scaler.fit_transform(song_features))\n",
    "# correlation = song_features.T.corr(method='pearson')\n",
    "\n",
    "# had to switch methods of calculating coorrelation since the dataset is too large\n",
    "nn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "nn.fit(song_features)\n",
    "distances, indices = nn.kneighbors(song_features, n_neighbors=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5d23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(uri):\n",
    "    try:\n",
    "        tags = song_info[song_info[\"spotify_id\"] == uri][\"tags\"].iloc[0].split(\",\")\n",
    "        tags = [tag.strip() for tag in tags]\n",
    "    except:\n",
    "        return set()\n",
    "    return set(tags)\n",
    "\n",
    "def get_similar_songs(uri, n=10):\n",
    "    '''\n",
    "    Get the n most similar songs to a given song.\n",
    "\n",
    "    There is extra emphasis on similar artists and similar tags,\n",
    "    specifically:\n",
    "    - total similarity = correlation * sqrt(# of similar tags)\n",
    "    - total similarity *= 1.2 if the song is by the same artist\n",
    "\n",
    "    All songs by the same artist are also included in the list\n",
    "    of similar songs, since users are more inclined to add\n",
    "    songs by similar artists.\n",
    "    '''\n",
    "    # they can come in both ways, but the datast only has after spotify:track:\n",
    "    if uri.startswith(\"spotify:track:\"):\n",
    "        uri = uri[14:]\n",
    "\n",
    "    # make sure we actually have a valid song\n",
    "    if uri not in uri_to_idx:\n",
    "        return None\n",
    "    \n",
    "    # get the song tags & index\n",
    "    tags = get_tags(uri)\n",
    "    idx = uri_to_idx[uri]\n",
    "\n",
    "    # get similar songs\n",
    "    similar_idxs = indices[idx][1:]\n",
    "    similarities = 1 - distances[idx][1:]\n",
    "\n",
    "    best = pd.DataFrame({\n",
    "        \"correlation\": similarities,\n",
    "        \"spotify_id\": merged_df.loc[similar_idxs, \"spotify_id\"].values,\n",
    "        \"artist\": merged_df.loc[similar_idxs, \"artist\"].values,\n",
    "        \"name\": merged_df.loc[similar_idxs, \"name\"].values\n",
    "    }, index=similar_idxs)\n",
    "    \n",
    "    # add all songs by the same artist\n",
    "    artist = merged_df.iloc[idx][\"artist\"]\n",
    "    artist_songs = merged_df[merged_df[\"artist\"] == artist][\"spotify_id\"].tolist()\n",
    "\n",
    "    # artist boosting?\n",
    "    # for song in artist_songs:\n",
    "    #     artist_song_idx = uri_to_idx[song]\n",
    "    #     # add song to best list\n",
    "    #     if artist_song_idx not in best.index and artist_song_idx != idx:\n",
    "    #         # best.at[artist_song_idx, \"correlation\"] = merged_df[idx].corr(merged_df[artist_song_idx], method='cosine')\n",
    "    #         best.at[artist_song_idx, \"correlation\"] = 1 - cosine_similarity(\n",
    "    #             song_features.iloc[[idx]],\n",
    "    #             song_features.iloc[[artist_song_idx]]\n",
    "    #         )[0][0]\n",
    "    #         best.at[artist_song_idx, \"spotify_id\"] = song\n",
    "    #         best.at[artist_song_idx, \"artist\"] = merged_df.iloc[artist_song_idx][\"artist\"]\n",
    "    #         best.at[artist_song_idx, \"name\"] = merged_df.iloc[artist_song_idx][\"name\"]\n",
    "\n",
    "    # just using correlation was giving bad songs, make sure the tags are similar too\n",
    "    for i in best.index:\n",
    "\n",
    "        # # add tag checking if applicable\n",
    "        # if len(tags) > 0:\n",
    "        #     song_tags = get_tags(merged_df.iloc[i][\"spotify_id\"])\n",
    "\n",
    "        #     # drop songs with 0 common tags\n",
    "        #     if len(tags.intersection(song_tags)) == 0:\n",
    "        #         best = best.drop(index=i)\n",
    "        #     else:\n",
    "        #         similar_tags = len(tags.intersection(song_tags))\n",
    "\n",
    "        #         # don't just multiply by the number of tags, use log to make it less extreme\n",
    "        #         best.at[i, \"total_score\"] = best.at[i, \"correlation\"] * (np.sqrt(similar_tags))\n",
    "\n",
    "        # # no tags, just use correlation\n",
    "        # else:\n",
    "        best.at[i, \"total_score\"] = best.at[i, \"correlation\"]\n",
    "\n",
    "        # people will tend to add more songs from the same artist, boost them by 20%\n",
    "        if merged_df.iloc[i][\"artist\"] == merged_df.iloc[idx][\"artist\"]:\n",
    "            best.at[i, \"total_score\"] *= 1.2\n",
    "\n",
    "    if len(best) == 0:\n",
    "        return None\n",
    "    \n",
    "    # only send back the largest n\n",
    "    best = best.nlargest(n, 'total_score')\n",
    "    return best\n",
    "\n",
    "def get_batch_similarity(uris, n=10):\n",
    "    '''\n",
    "    Get the similarity for multiple songs and merge them into one\n",
    "    '''\n",
    "    total = pd.DataFrame(columns=[\"correlation\", \"spotify_id\", \"artist\", \"name\", \"total_score\", \"frequency\"])\n",
    "    for uri in uris:\n",
    "        # get the best songs\n",
    "        best = get_similar_songs(uri, n)\n",
    "\n",
    "        # song wasn't in the music info, continue\n",
    "        if best is None:\n",
    "            continue\n",
    "\n",
    "        # add the reuslts to the totals\n",
    "        for idx, row in best.iterrows():\n",
    "            if idx not in total.index:\n",
    "                total.loc[idx] = row\n",
    "                total.at[idx, \"frequency\"] = 1\n",
    "            else:\n",
    "                total.at[idx, \"total_score\"] += row[\"total_score\"]\n",
    "                total.at[idx, \"correlation\"] += row[\"correlation\"]\n",
    "                total.at[idx, \"frequency\"] += 1\n",
    "\n",
    "\n",
    "    return total.sort_values(by=\"total_score\" , ascending=False)\n",
    "\n",
    "# display(get_batch_similarity([\"0EFlTGZ6wsxK1kqhMc0kFY\", \"0gUA1gzM82FMySLD4rYQFf\", \"2oC0Z3n5w9KUNoHecdzfMV\"], 100))\n",
    "# display(get_similar_songs(\"0EFlTGZ6wsxK1kqhMc0kFY\", 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afbaff8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [13:21,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "fp = open('submission.hdf')\n",
    "out_fp = open('item_item_submission.hdf', 'w')\n",
    "\n",
    "i = 0\n",
    "for line in tqdm.tqdm(fp):\n",
    "    write_line = [i]\n",
    "    tracks = list(set(challenge_set_songs[i] + list(map(lambda x: x[1:], line.split(\",\")))[1:]))\n",
    "    most_similar = get_batch_similarity(tracks, 100)\n",
    "    write_line += list(most_similar[\"spotify_id\"].values)\n",
    "    out_fp.write(\", \".join(map(str, write_line)) + \"\\n\")\n",
    "fp.close()\n",
    "out_fp.close()\n",
    "\n",
    "# display(get_batch_similarity(tracks, 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci5123-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
